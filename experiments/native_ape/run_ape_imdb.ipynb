{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from output_parsing.APE_to_notebook import parse_ape_solutions, solution_to_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "APE_PATH = Path('..') / '..' / 'APE' / 'APE-2.0.3-executable.jar'\n",
    "USE_CASE_PATH = Path('.').resolve() / 'usecases' / 'imbd'\n",
    "config = 'config_run.json'\n",
    "input_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"ontology_path\": \"../../ontology/ontology_v2_DIM_2.owl\",\n",
    "    \"ontologyPrefixIRI\": \"http://www.co-ode.org/ontologies/ont.owl#\",\n",
    "    \"toolsTaxonomyRoot\": \"ToolsTaxonomy\",\n",
    "    \"dataDimensionsTaxonomyRoots\": [\n",
    "        \"DataClass\",\n",
    "        \"StatisticalRelevance\"\n",
    "    ],\n",
    "    \"tool_annotations_path\": \"../../ontology/tool_annotations_v2_DIM_2.json\",\n",
    "    \"constraints_path\": \"constraints_run.json\",\n",
    "    \"solutions_dir_path\": \"./solutions/\",\n",
    "    \"solution_length\": {\n",
    "        \"min\": 2,\n",
    "        \"max\": 10\n",
    "    },\n",
    "    \"solutions\": \"5\",\n",
    "    \"timeout_sec\": \"1000\",\n",
    "    \"number_of_execution_scripts\": \"0\",\n",
    "    \"number_of_generated_graphs\": \"5\",\n",
    "    \"tool_seq_repeat\": \"true\",\n",
    "    \"debug_mode\": \"false\",\n",
    "    \"use_workflow_input\": \"ONE\",\n",
    "    \"use_all_generated_data\": \"NONE\",\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"DataClass\": [\n",
    "                \"StrColumn\"\n",
    "            ],\n",
    "            \"StatisticalRelevance\": [\n",
    "                \"DependentVariable\"\n",
    "            ],\n",
    "            \"APE_label\": [\n",
    "                \"sentiment\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"DataClass\": [\n",
    "                \"StrColumn\"\n",
    "            ],\n",
    "            \"StatisticalRelevance\": [\n",
    "                \"IndependentVariable\"\n",
    "            ],\n",
    "            \"APE_label\": [\n",
    "                \"review\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"DataClass\": [\n",
    "                \"MixedDataFrame\"\n",
    "            ],\n",
    "            \"StatisticalRelevance\": [\n",
    "                \"NoRelevance\"\n",
    "            ],\n",
    "            \"APE_label\": [\n",
    "                \"imbd_train\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_MAPPING = [{\n",
    "    'label': 'imbd_train',\n",
    "    'source': os.path.abspath(os.path.join(\n",
    "        'usecases',\n",
    "        'imbd',\n",
    "        'imbd_train_fixed.csv', # ! change to fixed version to speed up demo, spelling correction takes too long\n",
    "    )),\n",
    "    'type': 'csv',\n",
    "    'DataClass': 'MixedDataFrame',\n",
    "    'StatisticalRelevance': 'NoRelevance'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_min_input = [\n",
    "    # Set 1 Preprocessing\n",
    "    (\n",
    "        [\n",
    "            {   # get_text_from_html_i\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['get_text_from_html_i']},\n",
    "                    {'APE_label': ['review']},\n",
    "                ]\n",
    "            }, { # expand_abbr_i\n",
    "                'constraintid': 'next_m',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['get_text_from_html_i']},\n",
    "                    {'ToolsTaxonomy': ['expand_abbr_i']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['expand_abbr_i']},\n",
    "                    {'APE_label': ['abrev.json']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['expand_abbr_i']},\n",
    "                    {'APE_label': ['review']},\n",
    "                ]\n",
    "            }, { # replace_re_i\n",
    "                'constraintid': 'next_m',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['expand_abbr_i']},\n",
    "                    {'ToolsTaxonomy': ['replace_re_i']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['replace_re_i']},\n",
    "                    {'APE_label': ['review']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['replace_re_i']},\n",
    "                    {'APE_label': ['[^a-zA-Z]']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['replace_re_i']},\n",
    "                    {'APE_label': [' ']},\n",
    "                ]\n",
    "            }, { # lemmatize_i\n",
    "                'constraintid': 'next_m',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['replace_re_i']},\n",
    "                    {'ToolsTaxonomy': ['lemmatize_i']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['lemmatize_i']},\n",
    "                    {'APE_label': ['review']},\n",
    "                ]\n",
    "            }, { # remove_stopwords_i\n",
    "                'constraintid': 'next_m',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['lemmatize_i']},\n",
    "                    {'ToolsTaxonomy': ['remove_stopwords_i']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['remove_stopwords_i']},\n",
    "                    {'APE_label': ['review']},\n",
    "                ]\n",
    "            },\n",
    "            # EDA\n",
    "            { # plot_wordcloud\n",
    "                'constraintid': 'last_m',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['plot_wordcloud']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['plot_wordcloud']},\n",
    "                    {'APE_label': ['review']},\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        6,\n",
    "        [\n",
    "            {\n",
    "                \"DataClass\": [\"Str\"],\n",
    "                \"StatisticalRelevance\": [\"BasicObjectRelevance\"],\n",
    "                \"APE_label\": [\"[^a-zA-Z]\"]\n",
    "            }, {\n",
    "                \"DataClass\": [\"Str\"],\n",
    "                \"StatisticalRelevance\": [\"BasicObjectRelevance\"],\n",
    "                \"APE_label\": [\" \"]\n",
    "            }, {\n",
    "                \"DataClass\": [\"Str\"],\n",
    "                \"StatisticalRelevance\": [\"BasicObjectRelevance\"],\n",
    "                \"APE_label\": [\"abrev.json\"]\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    # Set 2 Embedding + Modeling + Evaluation\n",
    "    (\n",
    "        [\n",
    "            { # column_split + train_test_split\n",
    "                'constraintid': 'connected_op',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['column_split']},\n",
    "                    {'ToolsTaxonomy': ['train_test_split']},\n",
    "                ]\n",
    "            }, { # embed_text_word2vec\n",
    "                'constraintid': 'connected_op',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['train_test_split']},\n",
    "                    {'ToolsTaxonomy': ['embed_text_word2vec']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'depend_m',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['embed_text_word2vec']},\n",
    "                    {'ToolsTaxonomy': ['train_test_split']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['embed_text_word2vec']},\n",
    "                    {'APE_label': ['review']},\n",
    "                ]\n",
    "            }, { # fit_estimator\n",
    "                'constraintid': 'connected_op',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['embed_text_word2vec']},\n",
    "                    {'ToolsTaxonomy': ['fit_estimator']},\n",
    "                ]\n",
    "            }, { # -> init_sklearn_estimator\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['fit_estimator']},\n",
    "                    {'DataClass': ['Classifier']},\n",
    "                ]\n",
    "            }, { # embed_text_word2vec + predict_estimator\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['embed_text_word2vec']},\n",
    "                    {'DataClass': ['Word2Vec']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'connected_op',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['embed_text_word2vec']},\n",
    "                    {'ToolsTaxonomy': ['predict']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'connected_op',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['fit_estimator']},\n",
    "                    {'ToolsTaxonomy': ['predict']},\n",
    "                ]\n",
    "            }, { # classification_report\n",
    "                'constraintid': 'operation_input',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['classification_report']},\n",
    "                    {'StatisticalRelevance': ['Prediction']},\n",
    "                ]\n",
    "            }, {\n",
    "                'constraintid': 'connected_op',\n",
    "                'parameters': [\n",
    "                    {'ToolsTaxonomy': ['train_test_split']},\n",
    "                    {'ToolsTaxonomy': ['classification_report']},\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        8,\n",
    "        [],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would like to replace word2vec hardcode with TextEmbedding but relations between multiple TextEmbedding operations + produced artifact are more complex. So, I'll leave it as is for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------\n",
      "\tWorkflow discovery - length 8\n",
      "-------------------------------------------------------------\n",
      "Total problem setup time: 38.432 sec (7290876 clauses).\n",
      "Found 5 solutions. Solving time: 18.562 sec.\n",
      "\n",
      "\n",
      "APE found 5 solutions.\n",
      "Total APE runtime: \t\t61.295 sec.\n",
      "Total encoding time: \t\t38.432 sec.\n",
      "Total SAT solving time: \t18.562 sec.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------\n",
      "\tGenerating graphical representation\n",
      "\tof the first 5 workflows\n",
      "-------------------------------------------------------------\n",
      "\n",
      "Loading.....\n",
      "\n",
      "Graphical files have been generated. Running time: 0.823 sec.\n",
      "CWL annotations file not configured. No executable CWL files are generated.\n",
      "APE finished 2\n",
      "Notebooks produced 2\n"
     ]
    }
   ],
   "source": [
    "last_len = -1\n",
    "for iter_ix, cst_conf in enumerate(constraint_min_input, start=1):\n",
    "    # if last_len >= 0:\n",
    "    #     input_step += last_len\n",
    "    #     print(f\"input_step: {input_step}\")\n",
    "    # else:\n",
    "    #     input_step = 0\n",
    "\n",
    "    # print('Iteration', iter_ix)\n",
    "\n",
    "    # if iter_ix != 1:\n",
    "    #     continue\n",
    "    if iter_ix != 2:\n",
    "        continue\n",
    "    input_step = 7\n",
    "\n",
    "    constraint_set, min_len, inputs = cst_conf\n",
    "\n",
    "    # create config and constraints\n",
    "    with open(USE_CASE_PATH / 'constraints_run.json', 'w', encoding='utf-8') as file_:\n",
    "        json.dump({\"constraints\": constraint_set}, file_, indent=4)\n",
    "\n",
    "    config_local = CONFIG.copy()\n",
    "    config_local['inputs'] += inputs\n",
    "    config_local['solution_length']['min'] = min_len\n",
    "    with open(USE_CASE_PATH / config, 'w', encoding='utf-8') as file_:\n",
    "        json.dump(config_local, file_, indent=4)\n",
    "\n",
    "    # run APE\n",
    "    proc = subprocess.Popen(\n",
    "        ['java', '-Xmx8g', '-jar', str(APE_PATH), config],\n",
    "        cwd=str(USE_CASE_PATH),\n",
    "    )\n",
    "    proc.wait()\n",
    "\n",
    "    # check error code\n",
    "    if proc.returncode != 0:\n",
    "        print('APE failed', iter_ix)\n",
    "        continue\n",
    "    print('APE finished', iter_ix)\n",
    "\n",
    "    # output\n",
    "    folder_path = USE_CASE_PATH / 'out' / f'iteration_{iter_ix}'\n",
    "    folder_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # copy input files into the output folder\n",
    "    shutil.copy(\n",
    "        str(USE_CASE_PATH / config),\n",
    "        str(folder_path / config),\n",
    "    )\n",
    "    shutil.copy(\n",
    "        str(USE_CASE_PATH / 'constraints_run.json'),\n",
    "        str(folder_path / 'constraints_run.json'),\n",
    "    )\n",
    "\n",
    "    # copy solutions\n",
    "    shutil.copy(\n",
    "        str(USE_CASE_PATH / 'solutions' / 'solutions.txt'),\n",
    "        str(folder_path / 'solutions.txt'),\n",
    "    )\n",
    "\n",
    "    shutil.copytree(\n",
    "        str(USE_CASE_PATH / 'solutions' / 'Figures'),\n",
    "        str(folder_path / 'Figures'),\n",
    "        dirs_exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # produce notebooks\n",
    "    workflows_list = parse_ape_solutions(\n",
    "        folder_path / 'solutions.txt',\n",
    "        input_step=input_step,\n",
    "    )\n",
    "\n",
    "    for wk_ix, workflow in enumerate(workflows_list, start=1):\n",
    "        notebook = solution_to_notebook(\n",
    "            workflow,\n",
    "            input_mapping=INPUT_MAPPING,\n",
    "            solution_num=wk_ix-1,\n",
    "            input_step=input_step,\n",
    "        )\n",
    "        with open(\n",
    "            folder_path / f'workflow_{wk_ix}_start_{input_step}.ipynb',\n",
    "            'w',\n",
    "            encoding='utf-8',\n",
    "        ) as out_:\n",
    "            json.dump(notebook, out_, indent=4)\n",
    "    last_len = len(workflows_list[-1]['steps'])\n",
    "    print('Notebooks produced', iter_ix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
